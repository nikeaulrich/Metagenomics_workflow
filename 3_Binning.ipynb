{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0afbdc78-2d19-413e-a8fd-02f0fd5f93de",
   "metadata": {},
   "source": [
    "# Binning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891df873-3aca-41b0-850c-4d7d1eb50dcf",
   "metadata": {},
   "source": [
    "Goal: use a combination of MetaBAT, MaxBin, CONCOCT. Then use DAS Tool to integrate the results of these binning algorithms to calculate an optimized, non-redundant set of bins from a single assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c357aa-1e65-4976-91a2-d41a00d28bfc",
   "metadata": {},
   "source": [
    "Software:\n",
    "\n",
    "MetaBAT2: https://bitbucket.org/berkeleylab/metabat/src/master/README.md\n",
    "\n",
    "CONCOCT: https://github.com/BinPro/CONCOCT; https://concoct.readthedocs.io/en/latest/usage.html\n",
    "\n",
    "CheckM: https://github.com/Ecogenomics/CheckM/wiki\n",
    "\n",
    "DAS_tool: https://github.com/cmks/DAS_Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f039b-bc62-4e6b-9603-f4fadbbc94d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALLATION\n",
    "module load conda/latest\n",
    "conda create -n binning python=3.7\n",
    "conda activate binning\n",
    "conda install -c bioconda metabat2\n",
    "conda install -c bioconda checkm-genome\n",
    "conda install -c bioconda das_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d97b86d-8223-42bc-bc26-599f7c036140",
   "metadata": {},
   "source": [
    "## Metabat2\n",
    "https://bitbucket.org/berkeleylab/metabat/src/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626e251-2e9c-4902-a52c-b103d09eabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=50G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 24:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/binning/mcav/slurm-metabat2binning-%j.out  # %j = job ID  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate binning\n",
    "\n",
    "#set parameters for binning:\n",
    "SAMPLENAME=\"mcav\"\n",
    "BINDIR=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/binning/${SAMPLENAME}/MetaBAT2_bins\"\n",
    "mkdir -p $BINDIR\n",
    "CONTIGPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/mapping/${SAMPLENAME}\"\n",
    "CONTIGFILE=\"${SAMPLENAME}.contigs-fixed.fsa\"\n",
    "\n",
    "#create depth file for MetaBat2\n",
    "jgi_summarize_bam_contig_depths --outputDepth $BINDIR/MetaBAT2_depth.txt $CONTIGPATH/*.bam\n",
    "\n",
    "#MetaBat2 script with verbose output, minimum length (m)(has to be >=1500) and no min bin size \n",
    "metabat2 -i $CONTIGPATH/$CONTIGFILE -a $BINDIR/MetaBAT2_depth.txt \\\n",
    "-o $BINDIR/metabat2 -m 1500\n",
    "\n",
    "# MetaBAT2 (v2:2.17)\n",
    "# default parameters:\n",
    "#-m [ --minContig ] arg (=2500)    Minimum size of a contig for binning (should be >=1500).\n",
    "#  --maxP arg (=95)                  Percentage of 'good' contigs considered for binning decided by connection\n",
    "#                                    among contigs. The greater, the more sensitive.\n",
    "#  --minS arg (=60)                  Minimum score of a edge for binning (should be between 1 and 99). The \n",
    "#                                    greater, the more specific.\n",
    "#  --maxEdges arg (=200)             Maximum number of edges per node. The greater, the more sensitive.\n",
    "#  --pTNF arg (=0)                   TNF probability cutoff for building TNF graph. Use it to skip the \n",
    "#                                    preparation step. (0: auto).\n",
    "#  -x [ --minCV ] arg (=1)           Minimum mean coverage of a contig in each library for binning.\n",
    "#  --minCVSum arg (=1)               Minimum total effective mean coverage of a contig (sum of depth over \n",
    "#                                    minCV) for binning.\n",
    "#  -s [ --minClsSize ] arg (=200000) Minimum size of a bin as the output.\n",
    "#  -t [ --numThreads ] arg (=0)      Number of threads to use (0: use all cores).\n",
    "\n",
    "#this runs CheckM immediately after and puts the results alongside your bins\n",
    "checkm lineage_wf -x fa -t 3 $BINDIR/ $BINDIR/checkm-bins-stats\n",
    "\n",
    "# JOB-ID: \n",
    "# bash script file name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32f653-cde7-4691-a2a4-9eab51fe1702",
   "metadata": {},
   "source": [
    "## CONCOCT\n",
    "https://concoct.readthedocs.io/en/latest/installation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac256f-63a0-4de1-8ca2-83cbd624c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALLATION (recommended it is installed in an isolated env)\n",
    "conda config --add channels defaults\n",
    "conda config --add channels bioconda\n",
    "conda config --add channels conda-forge\n",
    "\n",
    "conda create -n concoct_env python=3 concoct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b98bf-edcd-4caa-8845-7708011c10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=50G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 24:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/binning/dlab/slurm-concoctbinning-%j.out  # %j = job ID  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate concoct_env\n",
    "\n",
    "#set parameters\n",
    "SAMPLENAME=\"dlab\"\n",
    "BINPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/binning/${SAMPLENAME}/CONCOCT_bins\"\n",
    "mkdir -p $BINPATH\n",
    "CONTIGPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/mapping/${SAMPLENAME}\"\n",
    "CONTIGFILE=\"${SAMPLENAME}.contigs-fixed.fsa\"\n",
    "BAMPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/mapping/${SAMPLENAME}\"\n",
    "\n",
    "TEMPDIR=\"/project/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL_files/binning/concoct_${SAMPLENAME}_temp\"\n",
    "mkdir -p $TEMPDIR\n",
    "\n",
    "#creates the CONCOCT depth file\n",
    "#this part cuts up the contigs into 10kb pieces for CONCOCT to use \n",
    "cut_up_fasta.py $CONTIGPATH/$CONTIGFILE -c 10000 -o 0 --merge_last -b $BINPATH/${SAMPLENAME}_contigs_cut.bed > $BINPATH/${SAMPLENAME}_contigs_cut.fa\n",
    "\n",
    "#estimate contig coverage\n",
    "concoct_coverage_table.py $BINPATH/${SAMPLENAME}_contigs_cut.bed $BAMPATH/*.bam > $BINPATH/coverage_table_${SAMPLENAME}.tsv || { echo 'Exit code 2: failed to create coverage file, exiting.' && exit; }\n",
    "\n",
    "#run CONCOCT\n",
    "concoct --composition_file $BINPATH/${SAMPLENAME}_contigs_cut.fa --coverage_file $BINPATH/coverage_table_${SAMPLENAME}.tsv -t 3 -b $TEMPDIR || { echo 'Exit code 3: CONCOCT failed to run, exiting.' && exit; }\n",
    "merge_cutup_clustering.py $TEMPDIR/clustering_gt1000.csv > $TEMPDIR/${SAMPLENAME}_clustering_merged.csv || { echo 'Exit code 4: failed to merge clusters, exiting.' && exit; }\n",
    "extract_fasta_bins.py $CONTIGPATH/$CONTIGFILE $TEMPDIR/${SAMPLENAME}_clustering_merged.csv --output_path $BINPATH || { echo 'Exit code 5: Bins were not extracted, exiting.' && exit; }\n",
    "\n",
    "# Checkm is in binning env so switching environments \n",
    "conda deactivate\n",
    "\n",
    "conda activate binning\n",
    "\n",
    "#this runs CheckM immediately after and puts the results alongside your bins\n",
    "checkm lineage_wf -x fa -t 3 $BINPATH  $BINPATH/CheckM_stats\n",
    "\n",
    "# JOB-ID:\n",
    "# bash script file name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91ac56-64e6-4b4e-8f80-c3fc25f16c13",
   "metadata": {},
   "source": [
    "## DAS Tool\n",
    "https://github.com/cmks/DAS_Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f2a6b-a767-447f-9f7c-bad4ce9ffde2",
   "metadata": {},
   "source": [
    "The binning conda env should already have das_tool installed (see top installation window). You will need to convert fasta output from MetaBat2 and CONCOCT into the correct format for DAS tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701d5809-8a45-455d-9e68-61ebb439b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9386a2-4c67-4123-88c1-91e464bc91da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fasta_to_Contig2Bin.sh -i MetaBAT2_bins/ -e fa > ./metabat2.contigs2bin.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaed806-f65e-47e1-b87d-ad2d65d66654",
   "metadata": {},
   "outputs": [],
   "source": [
    "perl -pe \"s/,/\\tconcoct./g;\" /project/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL_files/binning/concoct_dlab_temp/dlab_clustering_merged.csv > ./concoct.contigs2bin.tsv\n",
    "#the clustering csv file is in the temp folder over in projects (hence the long file path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cf6b96-6b26-41fa-9070-4568626f7cc6",
   "metadata": {},
   "source": [
    "*Download concoct.contigs2bin.tsv and remove the first row (heading) and then re-upload!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d268c-40a2-4677-84f9-ff13e9938efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=50G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 24:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/binning/dlab/slurm-das_tool-%j.out  # %j = job ID  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate binning\n",
    "\n",
    "# Set parameters\n",
    "SAMPLENAME=\"dlab\"\n",
    "CONCOCTPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/binning/${SAMPLENAME}/concoct.contigs2bin.tsv\"  \n",
    "METABATPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/binning/${SAMPLENAME}/metabat2.contigs2bin.tsv\"\n",
    "CONTIGPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/mapping/${SAMPLENAME}\"\n",
    "CONTIGFILE=\"${SAMPLENAME}.contigs-fixed.fsa\"\n",
    "OUTDIR=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/binning/${SAMPLENAME}\"\n",
    "\n",
    "#Run DAS Tool\n",
    "DAS_Tool -i $CONCOCTPATH,$METABATPATH \\\n",
    "-l concoct,metabat2 \\\n",
    "-c $CONTIGPATH/$CONTIGFILE \\\n",
    "-t 11 \\\n",
    "--write_bin_evals \\\n",
    "--write_bins \\\n",
    "-o $OUTDIR/\"${SAMPLENAME}\"\n",
    "\n",
    "# -i input list: tab seperated table of contigs-bins \n",
    "#--score_threshold default is 0.5\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "BINPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/binning/${SAMPLENAME}/${SAMPLENAME}_DASTool_bins\"\n",
    "\n",
    "#Run CheckM\n",
    "checkm lineage_wf -x fa -t 3 $BINPATH $BINPATH/CheckM_stats\n",
    "\n",
    "# JOB-ID:\n",
    "# bash script file name: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
