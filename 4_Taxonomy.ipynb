{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9706dd-813a-47c8-9923-639892f27a75",
   "metadata": {},
   "source": [
    "# Classifying Taxonomy and MAG abundances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d564a-cc43-429a-8439-35ec2fe2708f",
   "metadata": {},
   "source": [
    "Using GTDB-Tk: https://github.com/Ecogenomics/GTDBTk?tab=readme-ov-file GTDB-Tk identifies a core set of genes in the MAG, and use these to compute a species phylogeny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e783c-1ec2-4581-802a-c7ecc27c791f",
   "metadata": {},
   "source": [
    "**The gtdb reference database needs 100GB space so do not create conda env in /home directory. Either create the env in /work or /scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f741f585-27a3-42d6-be09-30e069fe3409",
   "metadata": {},
   "source": [
    "I used this opportunity to create a /scratch directory with this documentation: https://docs.unity.rc.umass.edu/documentation/managing-files/hpc-workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3aebb6-2f9c-454a-9be9-c85ab63e67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create scratch directory\n",
    "ws_allocate gtdb 30\n",
    "\n",
    "#FOR LATER:to release scratch space when you don't need it anymore\n",
    "ws_release gtdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415afb3-d5ec-49c4-8a7a-61edd752d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Info: creating workspace.\n",
    "/scratch3/workspace/nikea_ulrich_uml_edu-gtdb\n",
    "remaining extensions  : 3\n",
    "remaining time in days: 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842912fc-3f2a-4f07-9f42-f0d5208b5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALLATION of gtdbtk in /scratch\n",
    "cd /scratch3/workspace/nikea_ulrich_uml_edu-gtdb\n",
    "module load conda/latest\n",
    "mkdir -p /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs\n",
    "conda create --prefix /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/taxonomy python=3.8\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/taxonomy\n",
    "conda install -c conda-forge -c bioconda gtdbtk=2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e80f62-7b7d-4518-871d-d43bc5a23bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda package is bundled with a script that will automatically download, and extract the GTDB-Tk reference data, simply run:\n",
    "download-db.sh\n",
    "#This will take awhile -- massive database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f2ce0-36df-448a-9c46-833aa8ba61e7",
   "metadata": {},
   "source": [
    "https://ecogenomics.github.io/GTDBTk/commands/classify_wf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b155a-8ae6-45e6-990e-28532460b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=150G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 48:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/taxonomy/mcav/slurm-gtdb_taxonomy-%j.out  # %j = job ID  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/taxonomy\n",
    "\n",
    "# Set parameters\n",
    "SAMPLENAME=\"mcav\"\n",
    "BINPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/binning/${SAMPLENAME}/${SAMPLENAME}_DASTool_bins\"\n",
    "OUTDIR=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/taxonomy/${SAMPLENAME}\"\n",
    "mkdir -p $OUTDIR\n",
    "\n",
    "#Run gtdb-tk\n",
    "gtdbtk classify_wf -x fa --skip_ani_screen \\\n",
    "--genome_dir $BINPATH/ --out_dir $OUTDIR/gtdb_out\n",
    "\n",
    "#This will process all genomes in the directory <my_genomes> using both bacterial and archaeal marker sets and place the results in <output_dir>.\n",
    "#Genomes must be in FASTA format (gzip with the extension .gz is acceptable)\n",
    "\n",
    "# JOB-ID:\n",
    "# bash script file name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf73f0-431f-40f2-9e5e-328f354e7495",
   "metadata": {},
   "source": [
    "To be able to look at trees from gtdb-tk (which uses FastTree) in itol, use this script. However better practice is to take the protein alignments and construct trees with IQTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb1aa9-7826-4ddc-bb4b-af3fba6fa236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=100G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 24:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/taxonomy/mcav/slurm-itol-%j.out  # %j = job ID  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/taxonomy\n",
    "\n",
    "SAMPLENAME=\"mcav\"\n",
    "WORKDIR=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/taxonomy/${SAMPLENAME}/gtdb_out/classify\"\n",
    "OUTDIR=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/taxonomy/${SAMPLENAME}/gtbd_trees_for_vis\"\n",
    "mkdir -p $OUTDIR\n",
    "\n",
    "#convert all classify trees to itol readable format\n",
    "gtdbtk convert_to_itol --input $WORKDIR/gtdbtk.bac120.classify.tree.4.tree --output $OUTDIR/itol.gtdbtk.classify.tree.4.tree\n",
    "\n",
    "gtdbtk convert_to_itol --input $WORKDIR/gtdbtk.bac120.classify.tree.6.tree --output $OUTDIR/itol.gtdbtk.classify.tree.6.tree\n",
    "\n",
    "gtdbtk convert_to_itol --input $WORKDIR/gtdbtk.bac120.classify.tree.8.tree --output $OUTDIR/itol.gtdbtk.classify.tree.8.tree\n",
    "\n",
    "gtdbtk convert_to_itol --input $WORKDIR/gtdbtk.backbone.bac120.classify.tree --output $OUTDIR/itol.gtdbtk.backbone.classify.tree\n",
    "\n",
    "# JOB-ID:\n",
    "# bash script file name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b565c124-7d59-4e10-afb5-30b594177a01",
   "metadata": {},
   "source": [
    "## IQ-Tree\n",
    "Infer a phylogeny with the multiple sequence alignment from gtdbtk output\n",
    "\n",
    "http://www.iqtree.org/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1c1f0-3b22-43c5-abdd-3295e75c5784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALLATION\n",
    "module load conda/latest\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/taxonomy\n",
    "conda install -c bioconda iqtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f7a324-92b7-4e6c-8148-12de6f2ccafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=100G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 24:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/taxonomy/slurm-iqtree-%j.out  # %j = job ID  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate /scratch3/workspace/nikea_ulrich_uml_edu-gtdb/envs/taxonomy\n",
    "\n",
    "#set parameters\n",
    "MSAPATH='/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/taxonomy/gtdb_allbins_out/align'\n",
    "MSA=\"gtdbtk.bac120.user_msa.fasta.gz\"\n",
    "OUTDIR=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/taxonomy/iqtree\"\n",
    "mkdir -p $OUTDIR\n",
    "\n",
    "cd $OUTDIR\n",
    "\n",
    "#run iqtree\n",
    "iqtree -s $MSAPATH/$MSA -T AUTO -m TEST -alrt 1000 -B 1000 --prefix all_bins\n",
    "\n",
    "# JOB-ID: \n",
    "# bash script file name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e339454-bab1-40db-8c5a-bda7ff0a8add",
   "metadata": {},
   "source": [
    "## CoverM \n",
    "https://wwood.github.io/CoverM/coverm-genome.html\n",
    "\n",
    "Map the reads of each sample back to the MAG catalogue and retrieve mapping statistics. This gives you relative abundance information to measure how abundant or rare each bacteria was in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ce4b3-55db-43a0-950b-95a081792d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install CoverM in the anvi'o environment \n",
    "module load conda/latest\n",
    "conda activate anvio-8\n",
    "conda install -c bioconda coverm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c5039d-d100-4f04-abd3-5dcc7d2728a8",
   "metadata": {},
   "source": [
    "**note: added bin identifier to each contig in MAG assembly files so that when they are concatenated into 1 MAG database, the MAGs can be identified still**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59814f35-c8e4-49d3-b4ed-c028a1f1ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -c 24  # Number of Cores per Task\n",
    "#SBATCH --mem=180G  # Requested Memory\n",
    "#SBATCH -p cpu  # Partition\n",
    "#SBATCH -t 24:00:00  # Job time limit\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH -o /work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/annotation/dlab/slurm-MAG-mapping-%j.out  # %j = job ID\n",
    "\n",
    "module load conda/latest\n",
    "conda activate anvio-8\n",
    "\n",
    "\n",
    "SAMPLENAME=\"dlab\"\n",
    "READSPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/assembly/${SAMPLENAME}/repaired\"\n",
    "MAGPATH=\"/scratch3/workspace/nikea_ulrich_uml_edu-gtdb/bins/${SAMPLENAME}\"\n",
    "WORKDIR=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/annotation/${SAMPLENAME}\"\n",
    "mkdir -p \"$WORKDIR\"\n",
    "MAGDB=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/annotation/${SAMPLENAME}/MAG_db\"\n",
    "mkdir -p \"$MAGDB\"\n",
    "XTRAFILES=\"/scratch3/workspace/nikea_ulrich_uml_edu-gtdb/annotation/${SAMPLENAME}\"\n",
    "mkdir -p \"$XTRAFILES\"\n",
    "LISTPATH=\"/work/pi_sarah_gignouxwolfsohn_uml_edu/nikea/COL/\"\n",
    "SAMPLELIST=\"032024_${SAMPLENAME}_sampleids.txt\"\n",
    "\n",
    "\n",
    "#concatenate MAGs into 1 file to create a MAG database \n",
    "#make sure MAG ID (bin#) has been addded to the beginning of each contig so that you can tell the bins apart\n",
    "cat $MAGPATH/*.fa > $MAGDB/\"$SAMPLENAME\"_mags.fsa\n",
    "\n",
    "cd $MAGDB\n",
    "#index MAG database with bowtie2\n",
    "bowtie2-build --large-index --threads 11 \"$SAMPLENAME\"_mags.fsa \"$SAMPLENAME\"_index\n",
    "\n",
    "while IFS= read -r SAMPLEID; do\n",
    "    #align reads to your contigs and collects that in a .sam file\n",
    "    bowtie2 --threads 11 -x \"$SAMPLENAME\"_index -1 $READSPATH/\"${SAMPLEID}\"_host_removed_R1.tagged_filter_ready.fastq.gz -2 $READSPATH/\"${SAMPLEID}\"_host_removed_R2.tagged_filter_ready.fastq.gz -S $XTRAFILES/\"${SAMPLEID}\".sam\n",
    "    #make sure to point it to the index not the FIXEDCON file (-x parameter)\n",
    "    \n",
    "    #converts your sam file to a bam file, but its neither sorted nor indexed, so we use an Anvi'O script to do so:\n",
    "    samtools view -F 4 -b -S $XTRAFILES/\"${SAMPLEID}\".sam -o $WORKDIR/\"${SAMPLEID}\"-RAW.bam\n",
    "   \n",
    "    #index and sort your bam file\n",
    "    anvi-init-bam $WORKDIR/\"${SAMPLEID}\"-RAW.bam -o $WORKDIR/\"${SAMPLEID}\".bam\n",
    "    \n",
    "    rm $WORKDIR/\"${SAMPLEID}\"-RAW.bam\n",
    "done < \"$LISTPATH/${SAMPLELIST}\"\n",
    "echo \"Mapping success!\"\n",
    "\n",
    "#extract stats with CoverM\n",
    "#relative abundance\n",
    "coverm genome \\\n",
    "    -b $WORKDIR/*.bam \\\n",
    "    -s . \\\n",
    "    -m relative_abundance \\\n",
    "    --min-covered-fraction 0 \\\n",
    "    -o $WORKDIR/mapping_rate\n",
    "\n",
    "#MAG coverage\n",
    "coverm genome \\\n",
    "    -b $WORKDIR/*.bam \\\n",
    "    -s . \\\n",
    "    -m count covered_fraction \\\n",
    "    --min-covered-fraction 0 \\\n",
    "    -o $WORKDIR/count_table\n",
    "\n",
    "#-s flag is the character (\".\") that separates the MAG ID from the rest of the contig info.  \n",
    "\n",
    "conda deactivate\n",
    "\n",
    "#JOB ID:\n",
    "#bash script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711167f1-58f3-41e8-b6dc-3cb1fd066e7e",
   "metadata": {},
   "source": [
    "use EnrichM for better taxonomic profiling of MAGs: https://github.com/geronimp/enrichM?tab=readme-ov-file ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
